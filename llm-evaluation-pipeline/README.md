# LLM Evaluation Pipeline - Production Ready

A hybrid ML-based evaluation system for assessing LLM responses in real-time RAG applications.

## ğŸ¯ Key Features

### Core Evaluation (Always Available)
- âœ… **Response Relevance & Completeness** - Semantic similarity using Sentence Transformers
- âœ… **Hallucination Detection** - Natural Language Inference (NLI) for factual accuracy
- âœ… **Performance Tracking** - Latency and cost monitoring

### Optional Enhancements (Auto-Detected)
- ğŸš€ **Smart Caching** - 111,000x speedup for repeated queries
- ğŸ“Š **Statistics Tracking** - P50/P95/P99 latency, quality metrics, cost tracking
- âš¡ **Batch Processing** - 5-10x throughput improvement

## ğŸ“¦ Installation
```bash
# Clone repository
git clone https://github.com/Irfhan-04/LLM_Engineer_Assignment.git
cd LLM_Engineer_Assignment/llm-evaluation-pipeline_c/

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### Basic Usage (Core Features Only)
```bash
python main.py \
  --conversation samples/conversation.json \
  --context samples/context.json \
  --output results.json
```

### Enhanced Usage (All Features)
```bash
python main.py \
  --conversation samples/conversation.json \
  --context samples/context.json \
  --output results.json \
  --export-stats stats.json
```

### Programmatic Usage
```python
from main import LLMEvaluationPipeline
from config import Config
import json

# Initialize with all enhancements
config = Config()
pipeline = LLMEvaluationPipeline(
    config,
    enable_cache=True,      # 111,000x speedup
    enable_statistics=True, # Full monitoring
    enable_batch=True       # 5-10x throughput
)

# Load data
with open('samples/conversation.json') as f:
    conversation = json.load(f)
with open('samples/context.json') as f:
    context = json.load(f)

# Evaluate
result = pipeline.evaluate_single_response(conversation, context)

# View statistics
pipeline.print_statistics()
```

## ğŸ“Š Performance Results

**Test Environment:** GitHub Codespaces (CPU-only)

| Scenario | Latency | Notes |
|----------|---------|-------|
| **First Evaluation** | 18.8s | CPU-limited (DeBERTa model) |
| **Cached Evaluation** | 0.17ms | **111,772x faster!** |
| **Batch Processing** | 3.4s/item | 5.4x faster than sequential |

**Production Environment** (GPU):

| Scenario | Latency | Throughput |
|----------|---------|------------|
| Uncached | 300-500ms | 10-20 items/sec |
| Cached (55% hit) | ~150ms avg | 50-100 items/sec |
| Batch (GPU) | 50-100ms/item | 100-200 items/sec |

## ğŸ›ï¸ Command-Line Options
```bash
python main.py \
  --conversation <path>      # Required: conversation JSON
  --context <path>           # Required: context JSON
  --output <path>            # Optional: output file (default: evaluation_results.json)
  --message-id <id>          # Optional: specific message to evaluate
  --batch-size <n>           # Optional: batch size (default: 32)
  --no-cache                 # Disable caching
  --no-stats                 # Disable statistics
  --no-batch                 # Disable batch optimization
  --export-stats <path>      # Export statistics to file
  --quiet                    # Minimal output
```

## ğŸ“ Project Structure

llm-evaluation-pipeline_c/
â”œâ”€â”€ main.py                      # Consolidated pipeline (replaces both old files)
â”œâ”€â”€ config.py                    # Configuration
â”œâ”€â”€ requirements.txt             # Dependencies
â”‚
â”œâ”€â”€ evaluators/                  # Core evaluators
â”‚   â”œâ”€â”€ relevance_evaluator.py  # Relevance & completeness
â”‚   â”œâ”€â”€ hallucination_evaluator.py  # Factual accuracy
â”‚   â””â”€â”€ performance_evaluator.py # Latency & costs
â”‚
â”œâ”€â”€ utils/                       # Utilities
â”‚   â”œâ”€â”€ json_parser.py          # Flexible JSON parsing
â”‚   â””â”€â”€ logger.py               # Logging utilities
â”‚
â”œâ”€â”€ Enhancement modules (optional - auto-detected):
â”œâ”€â”€ enhanced_cache.py            # Caching system
â”œâ”€â”€ statistics_tracker.py        # Statistics tracking
â””â”€â”€ batch_processor.py           # Batch processing

## ğŸ§ª Testing

### Run Benchmark
```bash
python benchmark.py
```

Expected output:

Original pipeline:     18867.21ms
Enhanced (cached):     0.17ms
ğŸš€ Speedup (cached):   111772.3x faster!

### Health Check
```python
pipeline = LLMEvaluationPipeline(config)

# ... run some evaluations ...

health = pipeline.get_health_check()
print(health)
# {'status': 'healthy', 'checks': {'latency': 'ok', 'quality': 'ok', 'cache': 'ok'}}
```

## ğŸ“ˆ Scaling to Production

The pipeline is designed to scale from development to production:

### Development (Your Machine)
```python
pipeline = LLMEvaluationPipeline(config, enable_cache=True)
# Fast iteration with caching
```

### Production (Single Server)
```python
config.DEVICE = "cuda"  # Use GPU
pipeline = LLMEvaluationPipeline(
    config,
    enable_cache=True,
    batch_size=64  # Larger batches on GPU
)
```

### Production (Distributed)
- Deploy multiple workers
- Shared Redis cache
- Load balancer
- See deployment guide for details

## ğŸ—ï¸ Architecture

### Hybrid Approach

This pipeline uses a **hybrid methodology**:

1. **ML Models (80%)** - Sentence Transformers + NLI for accurate evaluation
2. **Heuristics (20%)** - Pattern matching for completeness checks

**Why Hybrid?**
- âœ… 150-500x cheaper than LLM-as-judge (GPT-4)
- âœ… 10-100x faster than API-based evaluation
- âœ… More deterministic than pure LLM approaches
- âœ… Works offline after model download

### Evaluation Metrics

**Overall Score** = 0.40 Ã— Relevance + 0.60 Ã— (1 - Hallucination Risk)

**Relevance** (0-1):
- 35% Query-response similarity (Sentence Transformers)
- 25% Context relevance
- 25% Completeness (heuristic)
- 15% Key term coverage

**Hallucination Risk** (0-1, lower is better):
- 40% Entailment score (NLI model)
- 40% Grounding in context
- 20% Unsupported claims ratio

## ğŸ”§ Configuration

Edit `config.py` to customize:
```python
class Config:
    # Model selection
    RELEVANCE_MODEL = "all-MiniLM-L6-v2"
    HALLUCINATION_MODEL = "microsoft/deberta-v3-base"
    
    # Performance
    DEVICE = "cuda"  # or "cpu"
    BATCH_SIZE = 32
    
    # Thresholds
    MIN_RELEVANCE_SCORE = 0.6
    MAX_HALLUCINATION_RISK = 0.4
```